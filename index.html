<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Jian Vora</title>
		<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
		<link rel="stylesheet" href="css/main.css">
		<style>
			* {
			   box-sizing: border-box;
			}
			.imageColumn {
			   float: left;
			   width: 25%;
			   padding: 10px;
			}
			h1{
			   text-align: center;
			}
			</style>
			
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-103320786-1', 'auto');
			ga('send', 'pageview');
		</script>
	</head>
	<body>
		<div class="container">
			<div class="page-header">
				<h2>Jian Vora 
				<p><small><a href="mailto:jianv@cs.stanford.edu">Email, </a><a href="https://github.com/jianvora">Github, </a><a href="https://www.linkedin.com/in/jianvora/">LinkedIn</a></small></h2></p>
				<div class="alignRow">
					<div class="imageColumn">
					<img src="images/me.jpg" alt="Snow" style="width:100%">
					</div>
					<div class="imageColumn">
					<img src="images/me2.jpg" alt="Forest" style="width:100%">
					</div>
				</div>
					
			
				<!-- <p class="profpic"><img src="images/me2.jpg" width="255px"
				/></p>
				<p class="profpic"><img src="images/me.jpg" width="255px"
				 /></p> -->
				<!-- <p><a href="https://github.com/jianvora">github</a>
				|Â <a href="https://www.linkedin.com/in/jianvora/">linkedin</a> -->

				</p>
			</div>

			<p>I am an MS student at Stanford in the Computer Science Department. Before that, I was a naive undergrad at IIT Bombay. I (pretend to) work on neural nonsense.
				 You can find me in one of my two natural eigenstates (images on the left).
			</p>

			<p>Very recently, I have been interested in LLM Agents, applications to scientific discovery, and shifting focus on evaluations of AI systems from AI models. I am also interested in the mathematical foundations of machine learning, which translates to learning and inferring over high-dimensions efficiently. In the past, I have worked on sparse recovery, tractable inference, and sequential decision-making problems. I have done some internships where I did funky deep learning.</p>

			<p> I am a SOTA neural net capable of (a) few-shot meta adaptation, (b) robust to adversarial attacks, and (c) in-context learning abilities. In my free time, I like to cook. </p>

<!-- 			<h3>Recent</h3>
			<p><strong>Reviewer</strong>: ICLR 2020, AAAI 2020, NeurIPS 2019, ICML 2019, ICLR 2019, R2L Workshop (NeurIPS 2018).<p> -->

			<!-- <dl>
				<dt>Neural Network Compression for Noisy Storage Devices.</dt>
				<dd>Berivan Isik, <b>Kristy Choi</b>, Xin Zheng, Tsachy Weissman, Stefano Ermon, H.-S. Philip Wong, Armin Alaghi</dd>
				<dd><i>arXiv preprint, 2021.</i></dd>
				<dd><a href="https://arxiv.org/abs/2102.07725">[arXiv]</a>[code soon]</dd>
			</dl> -->
<!-- 			<dl>
				<dt>Tensor Decomposition for Single-cell RNA-seq Data.</dt>
				<dd><b>Kristy Choi*</b>, Ambrose J. Carr*, Sandhya Prabhakaran, Dana Pe'er</dd>
				<dd><i>Practical Bayesian Nonparametrics Workshop, NeurIPS 2016. </i></dd>
				<dd><a href="https://drive.google.com/file/d/0B3WHb3BabixAb2RfMmc4eUJjUE0/view">[pdf]</a></dd>
			</dl> -->

			<h3>Conference Publications & Workshop Papers</h3>

			<dl>
				<dt><a href="https://openreview.net/pdf?id=kXlTY0BmK3">Benchmarking Large Language Models as AI Research Agents
				</a></dt>
				<dd>Qian Huang, <b>Jian Vora</b>, Percy Liang, Jure Leskovec</dd>
				<dd>can LLM Agents do AI Research: No but pretty good at ML engineering tasks</dd>
			</dl>

			<dl>
				<dt><a href="data/Twitch.pdf">Sequential Consumption-Aware Ranking Model for
					Recommendations at Twitch</a></dt>
				<dd><b>Jian Vora</b>, Edgar Chen, Nikita Mishra, Saad Ali</dd>
				<dd>leverage sequential user interaction data to improve ranking in recommendations</dd>
			</dl>

			<dl>
				<dt><a href="https://openreview.net/pdf?id=421Y6z81UZ">GNN Predictions on k-hop Egonets Boosts Adversarial
					Robustness</a></dt>
				<dd><b>Jian Vora</b></dd>
				<dd>make GNNs adversarially robust in under 5 lines of code</dd>
			</dl>

			<dl>
				<dt><a href="https://arxiv.org/pdf/2210.17140.pdf">Scoring Black-Box Models for Adversarial Robustness</a></dt>
				<dd><b>Jian Vora</b>, Pranay Reddy Samala</dd>
				<dd>adversarially robust models have sharper explanations and sparser lime weights, use this as a good subsitute for robust accuracy where trying to find attacks to the model can be hard</dd>
			</dl>

			<dl>
				<dt><a href="https://proceedings.mlr.press/v151/anand-jain22a/anand-jain22a.pdf">PAC Mode estimation using PPR Martingale Confidence Sequences</a></dt>
				<dd> Shubham Jain, Rohan Shah, Sanit Gupta*, Denil Mehta*, Inderjeet Nair*, <b>Jian Vora*</b>, Sushil Khyalia, Sourav Das, Vinay Riberio, Shivaram Kalyankrishnan</dd>
				<dd> asymptotically optimal mode estimation of a discrete distribution by construcing confidence sequences (1v1, 1vr); applications to election polls and contract verification in blockchains </dd>
			</dl>

			<dl>
				<dt><a href="https://ieeexplore.ieee.org/abstract/document/9513818">Recovery of Joint Probability Distribution from
					one-way marginals: Low Rank Tensors and Random Projections</a></dt>
				<dd> <b>Jian Vora</b>, Karthik Gurumoorthy, Ajit Rajwade</dd>
				<dd>model a joint pmf as a low-rank tensor, recover the mode factors from 1D marginals estimated from random projections of data </dd>
			</dl>

			<dl>
				<dt><a href="https://ieeexplore.ieee.org/abstract/document/9413470">Compressive signal recovery under sensing matrix errors combined
					with unknown measurement gains</a></dt>
				<dd> <b>Jian Vora</b>, Ajit Rajwade</dd>
				<dd>compressive recovery when the sensing matrix is misspecified and there are unknown sensor gains </dd>
			</dl>

			<h3>Preprints</h3>

			<dl>
				<dt><a href="data/PPPC.pdf">Plug&Play Multimodal Generative model
					allowing tractable inference</a></dt>
				<dd><b>Jian Vora</b>, Isabel Valera, Guy Van den Broeck, Antonio Vergari</dd>
				<dd>learn a joint distribution over multiple modalities while allowing for efficient marginalization, conditioning, likelihood evaluation using probabilisitc circuits on the fused latent space </dd>
			</dl>
			


			<h3>Other selected research + course projects</h3>
			<dl>
				<dt><a href="data/SRE_Report_JianVora.pdf">Efficient learning of log-concave mixtures</a></dt>
				<dd><b>Jian Vora</b>, Vivek Borkar</dd>
				<dd>random projections of data drawn from a mixture of log-concave densities are provably distributed as a gaussian mixture in the subspace</dd>
			</dl>

			<dl>
				<dt><a href="data/CS333_Final_Report.pdf">Tractable Cooperative Multi-Agent Reinforcement Learning</a></dt>
				<dd> learn a joint policy over actions of all agents allowing for efficient inference by modeling q-function to be a factor graph
					</dd>
			</dl>

			<dl>
				<dt><a href="data/CS_236_Final_Report.pdf">Improving Inference in latent variable models</a></dt>
				<dd> improved inference in VAEs by reducing two gaps -- approximation gap by using hierarchical VAEs and amortization gap by performing unamortized inference
					</dd>
			</dl>


			<dl>
				<dt><a href="https://www.researchgate.net/publication/337566411_NII_Hitachi_UIT_at_TRECVID_2019">Spatio-Temporal Action Detection and Classification</a></dt>
				<dd>participated in the trecvid'19 challenge which involved performing action detection and classification in videos. proposed a stage-wise architecture of object detection followed by tracking and activity classification
					</dd>
			</dl>

			<dl>
				<dt><a href="data/ASR_Report.pdf">Continual Learning for Keyword Spotting and Speaker Identification</a></dt>
				<dd>proposed a joint model to perform simultaneous kws and sid based on an interspeech 2021 challenge</dd>
			</dl>

			<dl>
				<dt><a href="data/EE782_StyleGAN.pdf">Conditional Style-GAN for audio generative modeling</a></dt>
				<dd>Modified stylegan to allow for conditioning and trained on audio spectrograms</dd>
			</dl>

			<h3>Teaching</h3>
			<p><strong>Summer 2023:</strong> Course Assistant
				for <a href="https://stanford-cs221.github.io/summer2023/">CS221: Artificial Intelligence</a> at Stanford<p>

			<p><strong>Winter 2023:</strong> Course Assistant
				for <a href="https://web.stanford.edu/class/cs234/">CS234: Reinforcement Learning</a> at Stanford<p>
	
			<p><strong>Fall 2022:</strong> Course Assistant
			for <a href="https://cs224v.stanford.edu/">CS224V: Conversational Virtual Assistants with Deep Learning</a> at Stanford<p>
			<p><strong>Winter 2022:</strong> Course Assistant
			for <a href="https://cs236g.stanford.edu/">CS236G: Generative Adversarial Networks</a> at Stanford<p>
			<p><strong>Spring 2021:</strong> Teaching Assistant
			for MA111: vector calculus at IITB<p>

			<h3>Service</h3>
			<p><strong>Reviewer</strong>: TMLR, IEEE TSP, AAAI, NeurIPS FMDM Workshop<p>
			<p><strong>Leadership</strong>: Manager, Electronics and Robotics club
			<br><br>
		</div>
	</body>
</html>
